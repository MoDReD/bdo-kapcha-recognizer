{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/app/data/store'\n",
    "\n",
    "classes = ['W','S','A','D','-']\n",
    "\n",
    "def vis_layers(folder, inputs):\n",
    "    \n",
    "    if not os.path.exists('./' + folder):\n",
    "        os.makedirs('./' + folder)\n",
    "        \n",
    "    outputs = []\n",
    "    names = []\n",
    "    for layer in modulelist[1:]:\n",
    "        inputs = layer(inputs)\n",
    "        outputs.append(inputs)\n",
    "        names.append(str(layer).partition('(')[0])\n",
    "\n",
    "    for i, name, image in zip(range(len(names)), names, outputs):\n",
    "        final = torch.cat((image[0].data), 0)\n",
    "        print(name, '  ', image[0].size(), '   ', final.size())\n",
    "        plt.imsave(fname='./{0}/{1}_{2}.png'.format(folder, i, name), arr=final, cmap='gray')\n",
    "        \n",
    "        \n",
    "def accuracy(inputs, labels):\n",
    "    success = 0\n",
    "    for i in range(10):\n",
    "        rez = []\n",
    "        for index, key in enumerate(inputs):\n",
    "            rez.append(key[i].max(0)[1].data[0])\n",
    "        l = labels[i].data.view(1,10)\n",
    "        f = torch.cuda.LongTensor(rez).view(1,10)\n",
    "        \n",
    "        if(torch.equal(l,f)): success+=1\n",
    "    \n",
    "    percent = success / len(inputs)\n",
    "    return percent\n",
    "\n",
    "\n",
    "def test_accuracy(train=True):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        inputs = Variable(data['image'].cuda())\n",
    "        labels = Variable(data['labels'].cuda())\n",
    "        layers = model(inputs)\n",
    "        final = layers['final']\n",
    "\n",
    "        total_acc += accuracy(final,labels)\n",
    "    model.train()\n",
    "    return total_acc / len(test_dataset)\n",
    "\n",
    "\n",
    "def name_classes(value):\n",
    "    a= ''\n",
    "    for i in range(len(value)):\n",
    "        a += classes[int(value[i])]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KapchaDataset(Dataset):\n",
    "    \"\"\"Kapcha dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, img_dir, csv_dir, length, first_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory.\n",
    "            img_dir  (string): Directory with all images.\n",
    "            csv_dir  (string): Directory with labels.\n",
    "            length      (int): Total length of dataset.\n",
    "            first_idx   (int): First data index.\n",
    "        \"\"\"\n",
    "        self.root_dir   = root_dir\n",
    "        self.img_dir    = os.path.join(root_dir, img_dir)\n",
    "        self.csv_dir    = os.path.join(root_dir, csv_dir)\n",
    "        self.first_idx  = first_idx\n",
    "        self.length     = length\n",
    "        \n",
    "        self.mean       = 0.079210128784179684\n",
    "        self.std        = 0.083445704142252608\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.first_idx + idx\n",
    "\n",
    "        img_name    = os.path.join(self.img_dir, '{0}.jpg'.format(idx))\n",
    "        image       = Image.open(img_name)\n",
    "        image       = np.array(image).astype('float32') / 255\n",
    "#         image       -= self.mean\n",
    "#         image       /= self.std\n",
    "        \n",
    "        image       = image[:, :, 0].reshape(1, 41, 265)\n",
    "        image       = torch.from_numpy(image)\n",
    "\n",
    "        label_name  = os.path.join(self.csv_dir, '{0}.csv'.format(idx))\n",
    "        labels      = np.genfromtxt(label_name, delimiter=',').astype('int64')\n",
    "        labels      = torch.from_numpy(labels)\n",
    "\n",
    "        sample = {'image': image, 'labels': labels, 'idx': idx}\n",
    "\n",
    "        return sample\n",
    "\n",
    "train_dataset   = KapchaDataset(root_dir=data_dir,\n",
    "                                img_dir='resized1',\n",
    "                                csv_dir='order',\n",
    "                                length=12000,\n",
    "                                first_idx=0)\n",
    "\n",
    "train_dataset   = DataLoader(train_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True,\n",
    "                             num_workers=0)\n",
    "\n",
    "test_dataset    = KapchaDataset(root_dir=data_dir,\n",
    "                                img_dir='resized1',\n",
    "                                csv_dir='order',\n",
    "                                length=2360,\n",
    "                                first_idx=12000)\n",
    "\n",
    "test_dataset    = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Parser, self).__init__()\n",
    "        \n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 3),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, 3),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 64, 3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "        )\n",
    "             \n",
    "        self.middle = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64 * 3 * 31, 1000),\n",
    "            torch.nn.BatchNorm1d(1000),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            \n",
    "            torch.nn.Linear(1000, 100),\n",
    "            torch.nn.BatchNorm1d(100),\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classification = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        features = self.features(inputs)\n",
    "        re_pool3 = features.view(-1 , 64 * 3 * 31)\n",
    "        \n",
    "        linear2 = self.middle(re_pool3)\n",
    "        \n",
    "        final = []\n",
    "        for index, layer in enumerate(self.classification):\n",
    "            final.append(layer(linear2))\n",
    "        \n",
    "        return {'final': final}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parser(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (middle): Sequential(\n",
       "    (0): Linear(in_features=5952, out_features=1000, bias=True)\n",
       "    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=1000, out_features=100, bias=True)\n",
       "    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (classification): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (1): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (2): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (3): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (4): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (5): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (6): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (7): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (8): Linear(in_features=100, out_features=5, bias=True)\n",
       "    (9): Linear(in_features=100, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Parser()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "    run_loss = 0\n",
    "    run_acc = 0\n",
    "    total_acc= 0\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        inputs = Variable(data['image'].cuda())\n",
    "        labels = Variable(data['labels'].cuda())\n",
    "        \n",
    "        layers = model(inputs)\n",
    "        final = layers['final']\n",
    "        \n",
    "        losses = []\n",
    "        for char_num in range(10):\n",
    "            loss = criterion(final[char_num], labels[:,char_num])\n",
    "            losses.append(loss)\n",
    "        \n",
    "        total_loss = sum(losses)\n",
    "        acc = accuracy(final,labels)\n",
    "        total_acc += acc\n",
    "        run_loss += total_loss\n",
    "        run_acc += acc\n",
    "        \n",
    "        if(i != 0 and i % 100 == 0):\n",
    "            print('{0} : {1}, Loss= {2}, Acc:{3}, TestAcc: {4}'.format(i, len(train_dataset), total_loss.data[0], total_acc / 100,test_accuracy()))\n",
    "            total_acc = 0\n",
    "            \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "    \n",
    "    print('Epoch: {0},  Loss:{1}, Acc:{2}, TestAcc: {3}'.format(epoch, run_loss.data[0] / len(train_dataset), run_acc / 1200,test_accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parser(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU(0.01)\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Conv2d (16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): LeakyReLU(0.01)\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (8): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): LeakyReLU(0.01)\n",
       "    (11): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (middle): Sequential(\n",
       "    (0): Linear(in_features=5952, out_features=1000)\n",
       "    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU(0.01)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=1000, out_features=100)\n",
       "    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): LeakyReLU(0.01)\n",
       "  )\n",
       "  (classification): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=5)\n",
       "    (1): Linear(in_features=100, out_features=5)\n",
       "    (2): Linear(in_features=100, out_features=5)\n",
       "    (3): Linear(in_features=100, out_features=5)\n",
       "    (4): Linear(in_features=100, out_features=5)\n",
       "    (5): Linear(in_features=100, out_features=5)\n",
       "    (6): Linear(in_features=100, out_features=5)\n",
       "    (7): Linear(in_features=100, out_features=5)\n",
       "    (8): Linear(in_features=100, out_features=5)\n",
       "    (9): Linear(in_features=100, out_features=5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './models/{0}.pt'.format('9_gpu'))\n",
    "model.cpu()\n",
    "torch.save(model.state_dict(), './models/{0}.pt'.format('9_cpu'))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/{0}.pt'.format('7_cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 16.26456069946289\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_dataset)\n",
    "data = it.next()\n",
    "          \n",
    "inputs = Variable(data['image'].cuda())\n",
    "labels = Variable(data['labels'].cuda())\n",
    "\n",
    "layers = model(inputs)\n",
    "\n",
    "final = layers['final']\n",
    "\n",
    "losses = []\n",
    "for char_num in range(10):\n",
    "    loss = criterion(final[char_num], labels[:,char_num])\n",
    "    losses.append(loss)\n",
    "\n",
    "total_loss = sum(losses)\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad() \n",
    "\n",
    "print('Total loss: {0}'.format(total_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Сохранение некорректно распознанных капч\"\"\"\n",
    "\n",
    "model.eval()\n",
    "for index, data in enumerate(train_dataset):\n",
    "    idx = data['idx']\n",
    "    inputs = Variable(data['image'].cuda())\n",
    "    labels = Variable(data['labels'].cuda())\n",
    "\n",
    "    layers = model(inputs)\n",
    "\n",
    "    final = layers['final']\n",
    "\n",
    "    for number in range(batch_size):\n",
    "        rez = []\n",
    "        for i in range(10):\n",
    "            rez.append(final[i][number].max(0)[1].data[0])\n",
    "\n",
    "        f = torch.cuda.LongTensor(rez)\n",
    "        if(not torch.equal(labels[number].data.view(1,10), f.view(1, 10))):\n",
    "            plt.imsave(fname='./errors/{0}_{1}.png'.format(idx[number],name_classes(f.view(10))), arr=inputs.data[number][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size torch.Size([10, 1, 58, 372])\n",
      "Conv2d     torch.Size([16, 56, 370])     torch.Size([896, 370])\n",
      "BatchNorm2d    torch.Size([16, 56, 370])     torch.Size([896, 370])\n",
      "LeakyReLU    torch.Size([16, 56, 370])     torch.Size([896, 370])\n",
      "MaxPool2d    torch.Size([16, 28, 185])     torch.Size([448, 185])\n",
      "Conv2d     torch.Size([32, 26, 183])     torch.Size([832, 183])\n",
      "BatchNorm2d    torch.Size([32, 26, 183])     torch.Size([832, 183])\n",
      "LeakyReLU    torch.Size([32, 26, 183])     torch.Size([832, 183])\n",
      "MaxPool2d    torch.Size([32, 13, 91])     torch.Size([416, 91])\n",
      "Conv2d     torch.Size([64, 11, 89])     torch.Size([704, 89])\n",
      "BatchNorm2d    torch.Size([64, 11, 89])     torch.Size([704, 89])\n",
      "LeakyReLU    torch.Size([64, 11, 89])     torch.Size([704, 89])\n",
      "MaxPool2d    torch.Size([64, 5, 44])     torch.Size([320, 44])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Визуализация слоев\"\"\"\n",
    "\n",
    "it = iter(train_dataset)\n",
    "\n",
    "value = it.next()\n",
    "value = Variable(value['image'].cuda())\n",
    "\n",
    "value.data.resize_(batch_size,1, 58, 372)\n",
    "print('input size',value.size())\n",
    "\n",
    "modulelist = list(model.features.modules())\n",
    "\n",
    "vis_layers('visual/layers/5', value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 3, 31])\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_dataset)\n",
    "data = it.next()\n",
    "          \n",
    "inputs = Variable(data['image'].cuda())\n",
    "labels = Variable(data['labels'].cuda())\n",
    "\n",
    "layers = model(inputs)\n",
    "print(layers.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
