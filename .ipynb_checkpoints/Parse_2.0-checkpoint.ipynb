{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/symbios/work/deep_learning/kapcha/data/store'\n",
    "\n",
    "classes = ['W','S','A','D','-']\n",
    "\n",
    "def save_im(conv, name):\n",
    "    final = torch.cat((conv.data), 0)\n",
    "    print(name, '  ', conv.size(), '   ', final.size())\n",
    "    plt.imsave(fname=name, arr=final, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KapchaDataset(Dataset):\n",
    "    \"\"\"Kapcha dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, img_dir, csv_dir, length, first_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory.\n",
    "            img_dir  (string): Directory with all images.\n",
    "            csv_dir  (string): Directory with labels.\n",
    "            length      (int): Total length of dataset.\n",
    "            first_idx   (int): First data index.\n",
    "        \"\"\"\n",
    "        self.root_dir   = root_dir\n",
    "        self.img_dir    = os.path.join(root_dir, img_dir)\n",
    "        self.csv_dir    = os.path.join(root_dir, csv_dir)\n",
    "        self.first_idx  = first_idx\n",
    "        self.length     = length\n",
    "#         self.mean       = (0.079210128784179684, 0.079210128784179684, 0.079210128784179684)\n",
    "#         self.std        = (0.083445704142252608, 0.083445704142252608, 0.083445704142252608)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.first_idx + idx\n",
    "\n",
    "        img_name    = os.path.join(self.img_dir, '{0}.jpg'.format(idx))\n",
    "        image       = Image.open(img_name)\n",
    "        image       = np.array(image).astype('float32') / 255\n",
    "        \n",
    "        image       = image[:, :, 0].reshape(1, 58, 372)\n",
    "        image       = torch.from_numpy(image)\n",
    "\n",
    "        label_name  = os.path.join(self.csv_dir, '{0}.csv'.format(idx))\n",
    "        labels      = np.genfromtxt(label_name, delimiter=',').astype('int64')\n",
    "        labels      = torch.from_numpy(labels)\n",
    "\n",
    "        sample = {'image': image, 'labels': labels, 'idx': idx}\n",
    "\n",
    "        return sample\n",
    "\n",
    "train_dataset   = KapchaDataset(root_dir=data_dir,\n",
    "                                img_dir='kapcha',\n",
    "                                csv_dir='order',\n",
    "                                length=12000,\n",
    "                                first_idx=0)\n",
    "\n",
    "train_dataset   = DataLoader(train_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True,\n",
    "                             num_workers=0)\n",
    "\n",
    "test_dataset    = KapchaDataset(root_dir=data_dir,\n",
    "                                img_dir='kapcha',\n",
    "                                csv_dir='order',\n",
    "                                length=2369,\n",
    "                                first_idx=12000)\n",
    "\n",
    "test_dataset    = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Parser, self).__init__()\n",
    "        self.pool = torch.nn.MaxPool2d(2, stride=2)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, 3)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 64, 3)\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(64 * 5 * 44, 1000)\n",
    "        self.linear2 = torch.nn.Linear(1000, 100)\n",
    "        \n",
    "        self.linear01 = torch.nn.Linear(100, 5)\n",
    "        self.linear02 = torch.nn.Linear(100, 5)\n",
    "        self.linear03 = torch.nn.Linear(100, 5)\n",
    "        self.linear04 = torch.nn.Linear(100, 5)\n",
    "        self.linear05 = torch.nn.Linear(100, 5)\n",
    "        self.linear06 = torch.nn.Linear(100, 5)\n",
    "        self.linear07 = torch.nn.Linear(100, 5)\n",
    "        self.linear08 = torch.nn.Linear(100, 5)\n",
    "        self.linear09 = torch.nn.Linear(100, 5)\n",
    "        self.linear10 = torch.nn.Linear(100, 5)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        conv1 = self.conv1(inputs)\n",
    "        relu1 = self.relu(conv1)\n",
    "        pool1 = self.pool(relu1)\n",
    "        \n",
    "        conv2 = self.conv2(pool1)\n",
    "        relu2 = self.relu(conv2)\n",
    "        pool2 = self.pool(relu2)\n",
    "        \n",
    "        conv3 = self.conv3(pool2)\n",
    "        relu3 = self.relu(conv3)\n",
    "        pool3 = self.pool(relu3)\n",
    "        \n",
    "        re_pool3 = pool3.view(-1 , 64 * 5 * 44)\n",
    "        \n",
    "        linear1 = self.linear1(re_pool3)\n",
    "        linear2 = self.linear2(linear1)\n",
    "\n",
    "        final = []\n",
    "        final.append(self.linear01(linear2))\n",
    "        final.append(self.linear02(linear2))\n",
    "        final.append(self.linear03(linear2))\n",
    "        final.append(self.linear04(linear2))\n",
    "        final.append(self.linear05(linear2))\n",
    "        final.append(self.linear06(linear2))\n",
    "        final.append(self.linear07(linear2))\n",
    "        final.append(self.linear08(linear2))\n",
    "        final.append(self.linear09(linear2))\n",
    "        final.append(self.linear10(linear2))\n",
    "        \n",
    "        \n",
    "        return {'1_conv1': conv1, '2_relu1': relu1, '3_pool1': pool1,\n",
    "                '4_conv2': conv2, '5_relu2': relu2, '6_pool2': pool2,\n",
    "                '7_conv3': conv3, '8_relu3': relu3, '9_pool3': pool3, 'final': final}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parser(\n",
       "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (relu): LeakyReLU(0.01)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (conv1): Conv2d (1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d (16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (linear1): Linear(in_features=14080, out_features=1000)\n",
       "  (linear2): Linear(in_features=1000, out_features=100)\n",
       "  (linear01): Linear(in_features=100, out_features=5)\n",
       "  (linear02): Linear(in_features=100, out_features=5)\n",
       "  (linear03): Linear(in_features=100, out_features=5)\n",
       "  (linear04): Linear(in_features=100, out_features=5)\n",
       "  (linear05): Linear(in_features=100, out_features=5)\n",
       "  (linear06): Linear(in_features=100, out_features=5)\n",
       "  (linear07): Linear(in_features=100, out_features=5)\n",
       "  (linear08): Linear(in_features=100, out_features=5)\n",
       "  (linear09): Linear(in_features=100, out_features=5)\n",
       "  (linear10): Linear(in_features=100, out_features=5)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Parser().cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=  1.6903685331344604\n",
      "Loss=  0.741338849067688\n",
      "Loss=  0.9532225131988525\n",
      "Loss=  0.8610985279083252\n",
      "Loss=  1.3867994546890259\n",
      "Loss=  1.8140556812286377\n",
      "Loss=  1.198352336883545\n",
      "Loss=  1.8545012474060059\n",
      "Loss=  1.5406296253204346\n",
      "Loss=  2.4570934772491455\n",
      "Loss=  1.6961561441421509\n",
      "Loss=  1.5229811668395996\n",
      "Epoch: 0 - Loss:1.5223494466145833\n",
      "Loss=  0.6402724981307983\n",
      "Loss=  1.6739414930343628\n",
      "Loss=  1.0715916156768799\n",
      "Loss=  0.922124981880188\n",
      "Loss=  0.9988433122634888\n",
      "Loss=  1.49813711643219\n",
      "Loss=  0.7800291776657104\n",
      "Loss=  1.0127769708633423\n",
      "Loss=  1.592955470085144\n",
      "Loss=  2.766164541244507\n",
      "Loss=  0.9748193621635437\n",
      "Loss=  1.860259771347046\n",
      "Epoch: 1 - Loss:1.3452581787109374\n",
      "Loss=  2.2971129417419434\n",
      "Loss=  1.127724051475525\n",
      "Loss=  0.5303621888160706\n",
      "Loss=  1.8541507720947266\n",
      "Loss=  1.4033986330032349\n",
      "Loss=  0.7157924771308899\n",
      "Loss=  0.9216243028640747\n",
      "Loss=  0.9056689739227295\n",
      "Loss=  1.0791168212890625\n",
      "Loss=  0.7970865368843079\n",
      "Loss=  0.8470706939697266\n",
      "Loss=  0.7485674023628235\n",
      "Epoch: 2 - Loss:1.1968776448567708\n",
      "Loss=  0.7394680976867676\n",
      "Loss=  0.9257513880729675\n",
      "Loss=  1.0442577600479126\n",
      "Loss=  1.0619510412216187\n",
      "Loss=  1.886975884437561\n",
      "Loss=  1.4068350791931152\n",
      "Loss=  1.1826496124267578\n",
      "Loss=  0.9183565378189087\n",
      "Loss=  0.2873668074607849\n",
      "Loss=  1.0974645614624023\n",
      "Loss=  0.30081477761268616\n",
      "Loss=  1.000040888786316\n",
      "Epoch: 3 - Loss:1.076082763671875\n",
      "Loss=  0.52105712890625\n",
      "Loss=  1.832671046257019\n",
      "Loss=  0.9466006755828857\n",
      "Loss=  0.35671132802963257\n",
      "Loss=  0.6530009508132935\n",
      "Loss=  0.7146026492118835\n",
      "Loss=  0.3665456771850586\n",
      "Loss=  1.5099701881408691\n",
      "Loss=  0.8319129347801208\n",
      "Loss=  0.47531431913375854\n",
      "Loss=  0.37088197469711304\n",
      "Loss=  0.8310473561286926\n",
      "Epoch: 4 - Loss:0.9717662556966146\n",
      "Loss=  1.1051808595657349\n",
      "Loss=  1.0727237462997437\n",
      "Loss=  0.5286394953727722\n",
      "Loss=  0.7757205963134766\n",
      "Loss=  1.1592282056808472\n",
      "Loss=  0.9474987387657166\n",
      "Loss=  0.7044536471366882\n",
      "Loss=  1.1564704179763794\n",
      "Loss=  1.0087668895721436\n",
      "Loss=  1.3040937185287476\n",
      "Loss=  0.5532485842704773\n",
      "Loss=  0.776803731918335\n",
      "Epoch: 5 - Loss:0.8861457316080729\n",
      "Loss=  0.2380484789609909\n",
      "Loss=  2.204587936401367\n",
      "Loss=  0.8208631873130798\n",
      "Loss=  0.6212747693061829\n",
      "Loss=  1.034616470336914\n",
      "Loss=  1.5666357278823853\n",
      "Loss=  2.71313738822937\n",
      "Loss=  0.12817604839801788\n",
      "Loss=  0.6825101971626282\n",
      "Loss=  1.4273351430892944\n",
      "Loss=  0.6479129195213318\n",
      "Loss=  2.082028388977051\n",
      "Epoch: 6 - Loss:0.8085216776529948\n",
      "Loss=  2.8173370361328125\n",
      "Loss=  0.27994364500045776\n",
      "Loss=  0.8555232286453247\n",
      "Loss=  1.411789894104004\n",
      "Loss=  1.0965156555175781\n",
      "Loss=  0.15446200966835022\n",
      "Loss=  0.2877490818500519\n",
      "Loss=  0.1979975551366806\n",
      "Loss=  0.8784019947052002\n",
      "Loss=  0.5897940993309021\n",
      "Loss=  1.0523645877838135\n",
      "Loss=  1.50810968875885\n",
      "Epoch: 7 - Loss:0.7418143208821615\n",
      "Loss=  0.49498093128204346\n",
      "Loss=  0.664966881275177\n",
      "Loss=  0.32814547419548035\n",
      "Loss=  0.6152262687683105\n",
      "Loss=  0.25410330295562744\n",
      "Loss=  0.2067229300737381\n",
      "Loss=  1.0741969347000122\n",
      "Loss=  0.37586358189582825\n",
      "Loss=  0.26364782452583313\n",
      "Loss=  0.45938634872436523\n",
      "Loss=  0.3721938729286194\n",
      "Loss=  0.623273491859436\n",
      "Epoch: 8 - Loss:0.6787503051757813\n",
      "Loss=  0.9105534553527832\n",
      "Loss=  0.1676129251718521\n",
      "Loss=  0.6275156140327454\n",
      "Loss=  0.5513396859169006\n",
      "Loss=  0.8852371573448181\n",
      "Loss=  0.581497311592102\n",
      "Loss=  0.8494475483894348\n",
      "Loss=  0.3204338550567627\n",
      "Loss=  0.6431183815002441\n",
      "Loss=  0.26697543263435364\n",
      "Loss=  0.35008272528648376\n",
      "Loss=  0.31003841757774353\n",
      "Epoch: 9 - Loss:0.627055918375651\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    run_loss = 0\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        inputs = Variable(data['image'].cuda())\n",
    "        labels = Variable(data['labels'].cuda())\n",
    "        \n",
    "        layers = model(inputs)\n",
    "        final = layers['final']\n",
    "        \n",
    "        losses = []\n",
    "        for char_num in range(10):\n",
    "            loss = criterion(final[char_num], labels[:,char_num])\n",
    "            losses.append(loss)\n",
    "\n",
    "        total_loss = sum(losses)\n",
    "        run_loss += total_loss\n",
    "        \n",
    "        if(i % 100 == 0):\n",
    "            print('Loss= ', total_loss.data[0])\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "    \n",
    "    print('Epoch: {0} - Loss:{1}'.format(epoch, run_loss.data[0]/len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 16.16602897644043\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_dataset)\n",
    "data = it.next()\n",
    "          \n",
    "inputs = Variable(data['image'].cuda())\n",
    "labels = Variable(data['labels'].cuda())\n",
    "\n",
    "layers = model(inputs)\n",
    "\n",
    "final = layers['final']\n",
    "\n",
    "losses = []\n",
    "for char_num in range(10):\n",
    "    loss = criterion(final[char_num], labels[:,char_num])\n",
    "    losses.append(loss)\n",
    "\n",
    "total_loss = sum(losses)\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad() \n",
    "\n",
    "print('Total loss: {0}'.format(total_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    2     0     2     3     2     4     4     4     4     4\n",
      "[torch.cuda.LongTensor of size 1x10 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "    4     3     3     4     3     3     4     3     2     4\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3\n",
    "\n",
    "it = iter(test_dataset)\n",
    "data = it.next()\n",
    "\n",
    "idx = data['idx']\n",
    "inputs = Variable(data['image'].cuda())\n",
    "labels = Variable(data['labels'].cuda())\n",
    "\n",
    "layers = model(inputs)\n",
    "\n",
    "final = layers['final']\n",
    "\n",
    "# print(final)\n",
    "rez = []\n",
    "for i in range(10):\n",
    "    rez.append(final[i][n].max(0)[1].data[0])\n",
    "    \n",
    "f = torch.FloatTensor(rez)\n",
    "\n",
    "print(labels[n].view(1,10))\n",
    "\n",
    "f.view(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./parse_layers/2_relu1.png    torch.Size([16, 56, 370])     torch.Size([896, 370])\n",
      "./parse_layers/4_conv2.png    torch.Size([32, 26, 183])     torch.Size([832, 183])\n",
      "./parse_layers/5_relu2.png    torch.Size([32, 26, 183])     torch.Size([832, 183])\n",
      "./parse_layers/6_pool2.png    torch.Size([32, 13, 91])     torch.Size([416, 91])\n",
      "./parse_layers/1_conv1.png    torch.Size([16, 56, 370])     torch.Size([896, 370])\n",
      "./parse_layers/3_pool1.png    torch.Size([16, 28, 185])     torch.Size([448, 185])\n",
      "./parse_layers/9_pool3.png    torch.Size([64, 5, 44])     torch.Size([320, 44])\n",
      "./parse_layers/8_relu3.png    torch.Size([64, 11, 89])     torch.Size([704, 89])\n",
      "./parse_layers/7_conv3.png    torch.Size([64, 11, 89])     torch.Size([704, 89])\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_dataset)\n",
    "\n",
    "idn = 0\n",
    "\n",
    "value = it.next()\n",
    "value = Variable(value['image'].cuda())\n",
    "\n",
    "value.data.resize_(batch_size,1, 58, 372)\n",
    "\n",
    "pred = model(value)\n",
    "\n",
    "for i, key in enumerate(pred):\n",
    "    save_im(pred[key][idn], './parse_layers/{1}.png'.format(i,key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
