{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/symbios/work/deep_learning/kapcha/data/store'\n",
    "\n",
    "classes = ['W','S','A','D','-']\n",
    "\n",
    "def vis_layers(folder, inputs):\n",
    "    \n",
    "    if not os.path.exists('./' + folder):\n",
    "        os.makedirs('./' + folder)\n",
    "        \n",
    "    outputs = []\n",
    "    names = []\n",
    "    for layer in modulelist[1:]:\n",
    "        inputs = layer(inputs)\n",
    "        outputs.append(inputs)\n",
    "        names.append(str(layer).partition('(')[0])\n",
    "\n",
    "    for i, name, image in zip(range(len(names)), names, outputs):\n",
    "        final = torch.cat((image[0].data), 0)\n",
    "        print(name, '  ', image[0].size(), '   ', final.size())\n",
    "        plt.imsave(fname='./{0}/{1}_{2}.png'.format(folder, i, name), arr=final, cmap='gray')\n",
    "        \n",
    "        \n",
    "def accuracy(inputs, labels):\n",
    "    success = 0\n",
    "    for i in range(10):\n",
    "        rez = []\n",
    "        for index, key in enumerate(inputs):\n",
    "            rez.append(key[i].max(0)[1].data[0])\n",
    "        l = labels[i].data.view(1,10)\n",
    "        f = torch.cuda.LongTensor(rez).view(1,10)\n",
    "        \n",
    "        if(torch.equal(l,f)): success+=1\n",
    "    \n",
    "    percent = success / len(inputs)\n",
    "    return percent\n",
    "\n",
    "\n",
    "def test_accuracy():\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    for i, data in enumerate(test_dataset):\n",
    "        inputs = Variable(data['image'].cuda())\n",
    "        labels = Variable(data['labels'].cuda())\n",
    "        layers = model(inputs)\n",
    "        final = layers['final']\n",
    "\n",
    "        total_acc += accuracy(final,labels)\n",
    "    model.train()\n",
    "    return total_acc / len(test_dataset)\n",
    "\n",
    "\n",
    "def name_classes(value):\n",
    "    a= ''\n",
    "    for i in range(len(value)):\n",
    "        a += classes[int(value[i])]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KapchaDataset(Dataset):\n",
    "    \"\"\"Kapcha dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, img_dir, csv_dir, length, first_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory.\n",
    "            img_dir  (string): Directory with all images.\n",
    "            csv_dir  (string): Directory with labels.\n",
    "            length      (int): Total length of dataset.\n",
    "            first_idx   (int): First data index.\n",
    "        \"\"\"\n",
    "        self.root_dir   = root_dir\n",
    "        self.img_dir    = os.path.join(root_dir, img_dir)\n",
    "        self.csv_dir    = os.path.join(root_dir, csv_dir)\n",
    "        self.first_idx  = first_idx\n",
    "        self.length     = length\n",
    "        \n",
    "        self.mean       = 0.079210128784179684\n",
    "        self.std        = 0.083445704142252608\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.first_idx + idx\n",
    "\n",
    "        img_name    = os.path.join(self.img_dir, '{0}.jpg'.format(idx))\n",
    "        image       = Image.open(img_name)\n",
    "        image       = np.array(image).astype('float32') / 255\n",
    "#         image       -= self.mean\n",
    "#         image       /= self.std\n",
    "        \n",
    "        image       = image[:, :, 0].reshape(1, 41, 265)\n",
    "        image       = torch.from_numpy(image)\n",
    "\n",
    "        label_name  = os.path.join(self.csv_dir, '{0}.csv'.format(idx))\n",
    "        labels      = np.genfromtxt(label_name, delimiter=',').astype('int64')\n",
    "        labels      = torch.from_numpy(labels)\n",
    "\n",
    "        sample = {'image': image, 'labels': labels, 'idx': idx}\n",
    "\n",
    "        return sample\n",
    "\n",
    "train_dataset   = KapchaDataset(root_dir=data_dir,\n",
    "                                img_dir='resized1',\n",
    "                                csv_dir='order',\n",
    "                                length=12000,\n",
    "                                first_idx=0)\n",
    "\n",
    "train_dataset   = DataLoader(train_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True,\n",
    "                             num_workers=0)\n",
    "\n",
    "test_dataset    = KapchaDataset(root_dir=data_dir,\n",
    "                                img_dir='resized1',\n",
    "                                csv_dir='order',\n",
    "                                length=2360,\n",
    "                                first_idx=12000)\n",
    "\n",
    "test_dataset    = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Parser, self).__init__()\n",
    "        \n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 3),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, 3),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 64, 3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, stride=2),\n",
    "        )\n",
    "             \n",
    "        self.middle = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64 * 1 * 21, 1000),\n",
    "            torch.nn.BatchNorm1d(1000),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            \n",
    "            torch.nn.Linear(1000, 100),\n",
    "            torch.nn.BatchNorm1d(100),\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.classification = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5),\n",
    "            torch.nn.Linear(100, 5)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        features = self.features(inputs)\n",
    "        re_pool3 = features.view(-1 , 64 * 1 * 21)\n",
    "        \n",
    "        linear2 = self.middle(re_pool3)\n",
    "        \n",
    "        final = []\n",
    "        for index, layer in enumerate(self.classification):\n",
    "            final.append(layer(linear2))\n",
    "        \n",
    "        return {'final': final}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parser(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU(0.01)\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Conv2d (16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): LeakyReLU(0.01)\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (8): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): LeakyReLU(0.01)\n",
       "    (11): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (middle): Sequential(\n",
       "    (0): Linear(in_features=1344, out_features=1000)\n",
       "    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU(0.01)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=1000, out_features=100)\n",
       "    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): LeakyReLU(0.01)\n",
       "  )\n",
       "  (classification): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=5)\n",
       "    (1): Linear(in_features=100, out_features=5)\n",
       "    (2): Linear(in_features=100, out_features=5)\n",
       "    (3): Linear(in_features=100, out_features=5)\n",
       "    (4): Linear(in_features=100, out_features=5)\n",
       "    (5): Linear(in_features=100, out_features=5)\n",
       "    (6): Linear(in_features=100, out_features=5)\n",
       "    (7): Linear(in_features=100, out_features=5)\n",
       "    (8): Linear(in_features=100, out_features=5)\n",
       "    (9): Linear(in_features=100, out_features=5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Parser().cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0,  Loss:1.4684585571289062, Acc:0.8076666666666624, TestAcc: 0.8953389830508492\n",
      "Epoch: 1,  Loss:1.4164515177408854, Acc:0.8138333333333276, TestAcc: 0.8822033898305106\n",
      "Epoch: 2,  Loss:1.3474184163411458, Acc:0.825666666666661, TestAcc: 0.8995762711864428\n",
      "Epoch: 3,  Loss:1.2920751953125, Acc:0.8362499999999928, TestAcc: 0.9199152542372901\n",
      "Epoch: 4,  Loss:1.2492757161458334, Acc:0.8351666666666605, TestAcc: 0.9076271186440698\n",
      "Epoch: 5,  Loss:1.199661865234375, Acc:0.8440833333333267, TestAcc: 0.925000000000002\n",
      "Epoch: 6,  Loss:1.154696553548177, Acc:0.8507499999999927, TestAcc: 0.9250000000000018\n",
      "Epoch: 7,  Loss:1.0965638224283853, Acc:0.8606666666666595, TestAcc: 0.924152542372883\n",
      "Epoch: 8,  Loss:1.0661079915364584, Acc:0.8636666666666616, TestAcc: 0.9364406779661034\n",
      "Epoch: 9,  Loss:1.0312753295898438, Acc:0.8676666666666599, TestAcc: 0.9411016949152559\n",
      "Epoch: 10,  Loss:0.9915709431966145, Acc:0.8758333333333271, TestAcc: 0.9423728813559338\n",
      "Epoch: 11,  Loss:0.9455376180013021, Acc:0.8829166666666592, TestAcc: 0.9512711864406793\n",
      "Epoch: 12,  Loss:0.9205587768554687, Acc:0.8836666666666599, TestAcc: 0.9474576271186458\n",
      "Epoch: 13,  Loss:0.8932243855794271, Acc:0.8866666666666606, TestAcc: 0.9449152542372896\n",
      "Epoch: 14,  Loss:0.8578670247395833, Acc:0.8905833333333262, TestAcc: 0.9559322033898316\n",
      "Epoch: 15,  Loss:0.8371208190917969, Acc:0.8969999999999935, TestAcc: 0.9559322033898318\n",
      "Epoch: 16,  Loss:0.8020299275716146, Acc:0.8992499999999933, TestAcc: 0.9478813559322049\n",
      "Epoch: 17,  Loss:0.7814898681640625, Acc:0.9014999999999952, TestAcc: 0.9495762711864421\n",
      "Epoch: 18,  Loss:0.7551719665527343, Acc:0.9066666666666617, TestAcc: 0.9542372881355945\n",
      "Epoch: 19,  Loss:0.7388444519042969, Acc:0.9070833333333291, TestAcc: 0.9563559322033911\n",
      "Epoch: 20,  Loss:0.7133008321126302, Acc:0.9096666666666613, TestAcc: 0.957627118644069\n",
      "Epoch: 21,  Loss:0.688085428873698, Acc:0.9127499999999946, TestAcc: 0.958898305084747\n",
      "Epoch: 22,  Loss:0.6738841756184896, Acc:0.913916666666661, TestAcc: 0.9601694915254247\n",
      "Epoch: 23,  Loss:0.6544171651204427, Acc:0.9191666666666627, TestAcc: 0.9601694915254249\n",
      "Epoch: 24,  Loss:0.6423992919921875, Acc:0.9178333333333281, TestAcc: 0.9614406779661028\n",
      "Epoch: 25,  Loss:0.6214595031738281, Acc:0.922499999999996, TestAcc: 0.96313559322034\n",
      "Epoch: 26,  Loss:0.603188222249349, Acc:0.9221666666666612, TestAcc: 0.9622881355932213\n",
      "Epoch: 27,  Loss:0.5799835713704428, Acc:0.9259166666666621, TestAcc: 0.9610169491525435\n",
      "Epoch: 28,  Loss:0.5761763000488281, Acc:0.9275833333333288, TestAcc: 0.9639830508474587\n",
      "Epoch: 29,  Loss:0.5571378072102865, Acc:0.9301666666666629, TestAcc: 0.9622881355932213\n",
      "Epoch: 30,  Loss:0.5440254211425781, Acc:0.9309166666666628, TestAcc: 0.9661016949152552\n",
      "Epoch: 31,  Loss:0.5247805786132812, Acc:0.9325833333333302, TestAcc: 0.9656779661016959\n",
      "Epoch: 32,  Loss:0.5175233968098958, Acc:0.9347499999999961, TestAcc: 0.9665254237288144\n",
      "Epoch: 33,  Loss:0.5068436686197917, Acc:0.9346666666666616, TestAcc: 0.9690677966101704\n",
      "Epoch: 34,  Loss:0.4878539530436198, Acc:0.9380833333333307, TestAcc: 0.9661016949152552\n",
      "Epoch: 35,  Loss:0.4838385009765625, Acc:0.9374166666666639, TestAcc: 0.9665254237288144\n",
      "Epoch: 36,  Loss:0.47445404052734375, Acc:0.9371666666666624, TestAcc: 0.9677966101694924\n",
      "Epoch: 37,  Loss:0.4620478312174479, Acc:0.9409999999999968, TestAcc: 0.9686440677966112\n",
      "Epoch: 38,  Loss:0.45098607381184896, Acc:0.94158333333333, TestAcc: 0.9703389830508482\n",
      "Epoch: 39,  Loss:0.4443047078450521, Acc:0.9424166666666639, TestAcc: 0.9703389830508484\n",
      "Epoch: 40,  Loss:0.42854039510091146, Acc:0.9451666666666644, TestAcc: 0.9699152542372891\n",
      "Epoch: 41,  Loss:0.42358202616373697, Acc:0.9459166666666643, TestAcc: 0.9711864406779671\n",
      "Epoch: 42,  Loss:0.4148430887858073, Acc:0.9454999999999967, TestAcc: 0.9716101694915263\n",
      "Epoch: 43,  Loss:0.4086262003580729, Acc:0.9453333333333314, TestAcc: 0.9716101694915263\n",
      "Epoch: 44,  Loss:0.3971294657389323, Acc:0.9489999999999958, TestAcc: 0.9733050847457636\n",
      "Epoch: 45,  Loss:0.3825446319580078, Acc:0.9508333333333314, TestAcc: 0.9737288135593228\n",
      "Epoch: 46,  Loss:0.3821710459391276, Acc:0.9505833333333313, TestAcc: 0.9733050847457635\n",
      "Epoch: 47,  Loss:0.37634719848632814, Acc:0.949249999999997, TestAcc: 0.9733050847457635\n",
      "Epoch: 48,  Loss:0.36600603739420573, Acc:0.9503333333333309, TestAcc: 0.9733050847457636\n",
      "Epoch: 49,  Loss:0.36198229471842447, Acc:0.9527499999999971, TestAcc: 0.9737288135593228\n",
      "Epoch: 50,  Loss:0.35379615783691404, Acc:0.9539999999999988, TestAcc: 0.9737288135593228\n",
      "Epoch: 51,  Loss:0.34689603169759115, Acc:0.9535833333333299, TestAcc: 0.9741525423728821\n",
      "Epoch: 52,  Loss:0.34516441345214843, Acc:0.9520833333333301, TestAcc: 0.9733050847457635\n",
      "Epoch: 53,  Loss:0.3265428415934245, Acc:0.9557499999999977, TestAcc: 0.97542372881356\n",
      "Epoch: 54,  Loss:0.32382568359375, Acc:0.9546666666666646, TestAcc: 0.9733050847457635\n",
      "Epoch: 55,  Loss:0.3181250254313151, Acc:0.9551666666666644, TestAcc: 0.97542372881356\n",
      "Epoch: 56,  Loss:0.30541737874348956, Acc:0.9584166666666649, TestAcc: 0.9766949152542379\n",
      "Epoch: 57,  Loss:0.30388165791829425, Acc:0.9584166666666655, TestAcc: 0.9741525423728821\n",
      "Epoch: 58,  Loss:0.304551264444987, Acc:0.9574166666666637, TestAcc: 0.976694915254238\n",
      "Epoch: 59,  Loss:0.30116661071777345, Acc:0.9559999999999974, TestAcc: 0.9758474576271193\n",
      "Epoch: 60,  Loss:0.2939615122477214, Acc:0.9580833333333311, TestAcc: 0.9762711864406786\n",
      "Epoch: 61,  Loss:0.2831616465250651, Acc:0.9607499999999988, TestAcc: 0.9758474576271193\n",
      "Epoch: 62,  Loss:0.2770159657796224, Acc:0.959999999999998, TestAcc: 0.9758474576271193\n",
      "Epoch: 63,  Loss:0.27616859436035157, Acc:0.9606666666666643, TestAcc: 0.9775423728813566\n",
      "Epoch: 64,  Loss:0.2665056610107422, Acc:0.9619166666666643, TestAcc: 0.9771186440677971\n",
      "Epoch: 65,  Loss:0.2681911468505859, Acc:0.9600833333333301, TestAcc: 0.9741525423728821\n",
      "Epoch: 66,  Loss:0.2605155436197917, Acc:0.961999999999999, TestAcc: 0.9758474576271193\n",
      "Epoch: 67,  Loss:0.25917666117350263, Acc:0.962749999999999, TestAcc: 0.9796610169491531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-19f9d92e3008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {0},  Loss:{1}, Acc:{2}, TestAcc: {3}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_acc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-28542da95379>\u001b[0m in \u001b[0;36mtest_accuracy\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'final'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_acc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-28542da95379>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(inputs, labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrez\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mpercent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccess\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    run_loss = 0\n",
    "    run_acc = 0\n",
    "    total_acc= 0\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        inputs = Variable(data['image'].cuda())\n",
    "        labels = Variable(data['labels'].cuda())\n",
    "        \n",
    "        layers = model(inputs)\n",
    "        final = layers['final']\n",
    "        \n",
    "        losses = []\n",
    "        for char_num in range(10):\n",
    "            loss = criterion(final[char_num], labels[:,char_num])\n",
    "            losses.append(loss)\n",
    "        \n",
    "        total_loss = sum(losses)\n",
    "        acc = accuracy(final,labels)\n",
    "        total_acc += acc\n",
    "        run_loss += total_loss\n",
    "        run_acc += acc\n",
    "        \n",
    "        if(i != 0 and i % 100 == 0):\n",
    "#             print('{0} : {1}, Loss= {2}, Acc:{3}, TestAcc: {4}'.format(i, len(train_dataset), total_loss.data[0], total_acc / 100,test_accuracy()))\n",
    "            total_acc = 0\n",
    "            \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "    \n",
    "    print('Epoch: {0},  Loss:{1}, Acc:{2}, TestAcc: {3}'.format(epoch, run_loss.data[0] / len(train_dataset), run_acc / 1200,test_accuracy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parser(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU(0.01)\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (4): Conv2d (16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): LeakyReLU(0.01)\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (8): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): LeakyReLU(0.01)\n",
       "    (11): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (middle): Sequential(\n",
       "    (0): Linear(in_features=14080, out_features=1000)\n",
       "    (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): LeakyReLU(0.01)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=1000, out_features=100)\n",
       "    (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (6): LeakyReLU(0.01)\n",
       "  )\n",
       "  (classification): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=5)\n",
       "    (1): Linear(in_features=100, out_features=5)\n",
       "    (2): Linear(in_features=100, out_features=5)\n",
       "    (3): Linear(in_features=100, out_features=5)\n",
       "    (4): Linear(in_features=100, out_features=5)\n",
       "    (5): Linear(in_features=100, out_features=5)\n",
       "    (6): Linear(in_features=100, out_features=5)\n",
       "    (7): Linear(in_features=100, out_features=5)\n",
       "    (8): Linear(in_features=100, out_features=5)\n",
       "    (9): Linear(in_features=100, out_features=5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './models/{0}.pt'.format('7_gpu'))\n",
    "model.cpu()\n",
    "torch.save(model.state_dict(), './models/{0}.pt'.format('7_cpu'))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/{0}.pt'.format('7_cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 16.26456069946289\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_dataset)\n",
    "data = it.next()\n",
    "          \n",
    "inputs = Variable(data['image'].cuda())\n",
    "labels = Variable(data['labels'].cuda())\n",
    "\n",
    "layers = model(inputs)\n",
    "\n",
    "final = layers['final']\n",
    "\n",
    "losses = []\n",
    "for char_num in range(10):\n",
    "    loss = criterion(final[char_num], labels[:,char_num])\n",
    "    losses.append(loss)\n",
    "\n",
    "total_loss = sum(losses)\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad() \n",
    "\n",
    "print('Total loss: {0}'.format(total_loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Сохранение некорректно распознанных капч\"\"\"\n",
    "\n",
    "model.eval()\n",
    "it = iter(test_dataset)\n",
    "\n",
    "for index, data in enumerate(test_dataset):\n",
    "    idx = data['idx']\n",
    "    inputs = Variable(data['image'].cuda())\n",
    "    labels = Variable(data['labels'].cuda())\n",
    "\n",
    "    layers = model(inputs)\n",
    "\n",
    "    final = layers['final']\n",
    "\n",
    "    for number in range(batch_size):\n",
    "        rez = []\n",
    "        for i in range(10):\n",
    "            rez.append(final[i][number].max(0)[1].data[0])\n",
    "\n",
    "        f = torch.cuda.LongTensor(rez)\n",
    "        if(not torch.equal(labels[number].data.view(1,10), f.view(1, 10))):\n",
    "            plt.imsave(fname='./errors/{0}_{1}.png'.format(idx[number],name_classes(f.view(10))), arr=inputs.data[number][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size torch.Size([10, 1, 58, 372])\n",
      "Conv2d     torch.Size([16, 56, 370])     torch.Size([896, 370])\n",
      "BatchNorm2d    torch.Size([16, 56, 370])     torch.Size([896, 370])\n",
      "LeakyReLU    torch.Size([16, 56, 370])     torch.Size([896, 370])\n",
      "MaxPool2d    torch.Size([16, 28, 185])     torch.Size([448, 185])\n",
      "Conv2d     torch.Size([32, 26, 183])     torch.Size([832, 183])\n",
      "BatchNorm2d    torch.Size([32, 26, 183])     torch.Size([832, 183])\n",
      "LeakyReLU    torch.Size([32, 26, 183])     torch.Size([832, 183])\n",
      "MaxPool2d    torch.Size([32, 13, 91])     torch.Size([416, 91])\n",
      "Conv2d     torch.Size([64, 11, 89])     torch.Size([704, 89])\n",
      "BatchNorm2d    torch.Size([64, 11, 89])     torch.Size([704, 89])\n",
      "LeakyReLU    torch.Size([64, 11, 89])     torch.Size([704, 89])\n",
      "MaxPool2d    torch.Size([64, 5, 44])     torch.Size([320, 44])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Визуализация слоев\"\"\"\n",
    "\n",
    "it = iter(train_dataset)\n",
    "\n",
    "value = it.next()\n",
    "value = Variable(value['image'].cuda())\n",
    "\n",
    "value.data.resize_(batch_size,1, 58, 372)\n",
    "print('input size',value.size())\n",
    "\n",
    "modulelist = list(model.features.modules())\n",
    "\n",
    "vis_layers('visual/layers/5', value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 1, 21])\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_dataset)\n",
    "data = it.next()\n",
    "          \n",
    "inputs = Variable(data['image'].cuda())\n",
    "labels = Variable(data['labels'].cuda())\n",
    "\n",
    "layers = model(inputs)\n",
    "print(layers.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
